{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load datasets\n",
    "def visualisation():\n",
    "    for i in range(19):\n",
    "        df = pd.read_csv('daliac/dataset_' + str(i+1) + '.txt', sep=',', header=None)\n",
    "    for i in range(1, 14):\n",
    "        df_exercise = df[df[24] == i].values\n",
    "        plt.plot(df_exercise[500:1500, :])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove noise\n",
    "def removeNoise():\n",
    "    b, a = signal.butter(4, 0.04, 'lowpass', analog=False)\n",
    "    for i in range(24):\n",
    "        df_exercise[:,i] = signal.lfilter(b, a, df_exercise[:, i])\n",
    "    plt.plot(df_exercise[500:1500, :])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.925778118680744 0.3197391374776279 -0.4858332689454794\n",
      "-0.19987240984775426 1.9906532360686415 0.7962199544500775\n",
      "-1.7709444962430094 0.29359669841436764 -0.4887516775618998\n",
      "-220.76413517379243 369.30496048463993 89.28096995942234\n",
      "-320.8695151202439 404.78867979228204 48.61833579157605\n",
      "-238.9855060432426 474.358805213539 8.86279077302094\n",
      "-0.3904758179492516 0.09286363627579444 -0.09570780900204111\n",
      "-2.4421721210093215 0.9088444635217665 -0.8286173153841635\n",
      "-0.7010040288315011 1.6312131255161466 0.49317535746973074\n",
      "-60.948018377397695 75.83585609436663 -0.9377025033183\n",
      "-35.631631377228324 21.41704942383088 -0.19239564138149517\n",
      "-14.839995339889624 16.703325638218615 -0.3093890146905477\n",
      "-2.7278867223972565 0.917996279334936 -0.9468731319901208\n",
      "-0.21825736581850055 0.1313342153844715 -0.05952275147529261\n",
      "-0.3014231858569701 0.772150661075941 0.2535961080727579\n",
      "-29.291308686773633 26.363366623058475 0.5846513854611051\n",
      "-28.735150662822438 27.04739073131202 1.0982116846163494\n",
      "-20.346804878939494 14.146324378640864 -0.26301765070853905\n",
      "-1.9760840349565607 0.4365707247307547 -0.8489099964295024\n",
      "-0.23451107199808927 0.651713846220283 0.24794804320597413\n",
      "-0.28943734299628365 0.2624614588592194 -0.011013364341891685\n",
      "-38.16625417429987 42.461043700545765 0.5791571933279901\n",
      "-96.43759464626099 125.75287473668102 11.587051272410587\n",
      "-156.3221220359727 148.35921489894113 -0.10092041307702129\n"
     ]
    }
   ],
   "source": [
    "seg = df_exercise[500:1500, 0:]\n",
    "for j in range(24):\n",
    "    dMin = np.min(seg[:, j])\n",
    "    dMax = np.max(seg[:, j])\n",
    "    dMean = np.mean(seg[:, j])\n",
    "\n",
    "    print(dMin, dMax, dMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureEngineering():    \n",
    "    for i in range(19):\n",
    "        df = pd.read_csv('daliac/dataset_' + str(i + 1) + '.txt', sep=',', header=None)\n",
    "        print('deal with dataset ' + str(i + 1))\n",
    "        for c in range(1, 14):\n",
    "            activity_data = df[df[24] == c].values\n",
    "            b, a = signal.butter(4, 0.04, 'low', analog=False)\n",
    "            for j in range(24):\n",
    "                activity_data[:, j] = signal.lfilter(b, a, activity_data[:, j])\n",
    "\n",
    "            training = np.empty(shape=(0, 10))\n",
    "\n",
    "            testing = np.empty(shape=(0, 10))\n",
    "            \n",
    "\n",
    "            datat_len = len(activity_data)\n",
    "            training_len = math.floor(datat_len * 0.8)\n",
    "            training_data = activity_data[:training_len, :]\n",
    "            testing_data = activity_data[training_len:, :]\n",
    "\n",
    "            # data segementation: for time series data, we need to segment the whole time series, and then extract features from each period of time\n",
    "            # to represent the raw data. In this example code, we define each period of time contains 1000 data points. Each period of time contains \n",
    "            # different data points. You may consider overlap segmentation, which means consecutive two segmentation share a part of data points, to \n",
    "            # get more feature samples.\n",
    "            training_sample_number = training_len // 1000 + 1\n",
    "            testing_sample_number = (datat_len - training_len) // 1000 + 1\n",
    "\n",
    "            for s in range(training_sample_number):\n",
    "                if s < training_sample_number - 1:\n",
    "                    sample_data = training_data[1000*s:1000*(s + 1), :]\n",
    "                else:\n",
    "                    sample_data = training_data[1000*s:, :]\n",
    "                # in this example code, only three accelerometer data in wrist sensor is used to extract three simple features: min, max, and mean value in\n",
    "                # a period of time. Finally we get 9 features and 1 label to construct feature dataset. You may consider all sensors' data and extract more\n",
    "\n",
    "                feature_sample = []\n",
    "                for i in range(3):\n",
    "                    feature_sample.append(np.min(sample_data[:, i]))\n",
    "                    feature_sample.append(np.max(sample_data[:, i]))\n",
    "                    feature_sample.append(np.mean(sample_data[:, i]))\n",
    "                feature_sample.append(sample_data[0, -1])\n",
    "                feature_sample = np.array([feature_sample])\n",
    "                training = np.concatenate((training, feature_sample), axis=0)\n",
    "            \n",
    "            for s in range(testing_sample_number):\n",
    "                if s < training_sample_number - 1:\n",
    "                    sample_data = testing_data[1000*s:1000*(s + 1), :]\n",
    "                else:\n",
    "                    sample_data = testing_data[1000*s:, :]\n",
    "\n",
    "                feature_sample = []\n",
    "                for i in range(3):\n",
    "                    feature_sample.append(np.min(sample_data[:, i]))\n",
    "                    feature_sample.append(np.max(sample_data[:, i]))\n",
    "                    feature_sample.append(np.mean(sample_data[:, i]))\n",
    "                feature_sample.append(sample_data[0, -1])\n",
    "                feature_sample = np.array([feature_sample])\n",
    "                testing = np.concatenate((testing, feature_sample), axis=0)\n",
    "\n",
    "    df_training = pd.DataFrame(training)\n",
    "    df_testing = pd.DataFrame(testing)\n",
    "    df_training.to_csv('training_data.csv', index=None, header=None)\n",
    "    df_testing.to_csv('testing_data.csv', index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_testing.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deal with dataset 1\n",
      "deal with dataset 2\n",
      "deal with dataset 3\n",
      "deal with dataset 4\n",
      "deal with dataset 5\n",
      "deal with dataset 6\n",
      "deal with dataset 7\n",
      "deal with dataset 8\n",
      "deal with dataset 9\n",
      "deal with dataset 10\n",
      "deal with dataset 11\n",
      "deal with dataset 12\n",
      "deal with dataset 13\n",
      "deal with dataset 14\n",
      "deal with dataset 15\n",
      "deal with dataset 16\n",
      "deal with dataset 17\n",
      "deal with dataset 18\n",
      "deal with dataset 19\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "#     visualisation()\n",
    "#     removeNoise()\n",
    "    featureEngineering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
