{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load datasets\n",
    "def visualisation():\n",
    "# load all daliac 19 datasets \n",
    "    for i in range(19):\n",
    "        df = pd.read_csv('daliac/dataset_' + str(i+1) + '.txt', sep=',', header=None)\n",
    "# get all activity data from all datasets\n",
    "    for i in range(1, 14):\n",
    "        df_exercise = df[df[24] == i].values\n",
    "        plt.plot(df_exercise[500:1500, :])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove noise\n",
    "def removeNoise():\n",
    "# Load dataset 15\n",
    "    df_15 = pd.read_csv('daliac/dataset_15.txt', sep=',', header=None)\n",
    "# select rope jumping values\n",
    "    df_rope = df_15[df_15[24] == 13].values\n",
    "# remove noise from rope jumping\n",
    "    b, a = signal.butter(4, 0.04, 'lowpass', analog=False)\n",
    "# retrive data from accelerometer on ankle    \n",
    "    for k in range(19,22):\n",
    "        df_rope[:,k] = signal.lfilter(b, a, df_rope[:, k])\n",
    "    plt.plot(df_rope[500:1500, 20:23])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureEngineering():\n",
    "    training = np.empty(shape=(0, 10))\n",
    "    testing = np.empty(shape=(0, 10))\n",
    "    # load all daliac 19 datasets \n",
    "    for i in range(19):\n",
    "        df = pd.read_csv('daliac/dataset_' + str(i + 1) + '.txt', sep=',', header=None)\n",
    "        print('deal with dataset ' + str(i + 1))\n",
    "        # get all activity data from all datasets\n",
    "        for c in range(1, 14):\n",
    "            activity_data = df[df[24] == c].values\n",
    "            # use Butterworth low pass filter to filter noise\n",
    "            b, a = signal.butter(4, 0.04, 'low', analog=False)\n",
    "            # apply noise filter to all activities\n",
    "            for j in range(24):\n",
    "                activity_data[:, j] = signal.lfilter(b, a, activity_data[:, j])            \n",
    "                \n",
    "                # get length of each data\n",
    "                datat_len = len(activity_data)\n",
    "                # divide data in 80% for training and 20% for testing\n",
    "                training_len = math.floor(datat_len * 0.8)\n",
    "                # set train and test data\n",
    "                training_data = activity_data[:training_len, :]\n",
    "                testing_data = activity_data[training_len:, :]\n",
    "                training_sample_number = training_len // 1000 + 1\n",
    "                testing_sample_number = (datat_len - training_len) // 1000 + 1\n",
    "                \n",
    "                # create sample training data\n",
    "                for s in range(training_sample_number):\n",
    "                    if s < training_sample_number - 1:\n",
    "                        sample_data = training_data[1000*s:1000*(s + 1), :]\n",
    "                        \n",
    "                    else:\n",
    "                        sample_data = training_data[1000*s:, :]\n",
    "                    feature_sample = []\n",
    "                    \n",
    "                    # retrieve x, y and z coordinate data from accelerometer on ankle sensor\n",
    "                    for i in range(19,22):\n",
    "                        feature_sample.append(np.min(sample_data[:, i])) # min value of x, y and z\n",
    "                        feature_sample.append(np.max(sample_data[:, i])) # max value of x, y and z\n",
    "                        feature_sample.append(np.mean(sample_data[:, i])) # mean value of x, y and z\n",
    "                    feature_sample.append(sample_data[0, -1])\n",
    "                    feature_sample = np.array([feature_sample])\n",
    "                    training = np.concatenate((training, feature_sample), axis=0)\n",
    "                        \n",
    "                # create sample testing data\n",
    "                for s in range(testing_sample_number):\n",
    "                    if s < training_sample_number - 1:\n",
    "                        sample_data = testing_data[1000*s:1000*(s + 1), :]\n",
    "                    else:\n",
    "                        sample_data = testing_data[1000*s:, :]\n",
    "        \n",
    "                    feature_sample = []\n",
    "                    # retrieve x, y and z coordinate data from accelerometer on ankle sensor\n",
    "                    for i in range(19,22):\n",
    "                        feature_sample.append(np.min(sample_data[:, i]))\n",
    "                        feature_sample.append(np.max(sample_data[:, i]))\n",
    "                        feature_sample.append(np.mean(sample_data[:, i]))\n",
    "                    feature_sample.append(sample_data[0, -1])\n",
    "                    feature_sample = np.array([feature_sample])\n",
    "                    testing = np.concatenate((testing, feature_sample), axis=0)\n",
    "                    \n",
    "    df_training = pd.DataFrame(training)\n",
    "    df_testing = pd.DataFrame(testing)\n",
    "    df_training.to_csv('training_data.csv', index=None, header=None)\n",
    "    df_testing.to_csv('testing_data.csv', index=None, header=None)\n",
    "    print(feature_sample)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelEvaluation():\n",
    "    # Read training and testing data\n",
    "    df_training = pd.read_csv('training_data.csv', header=None)\n",
    "    df_testing = pd.read_csv('testing_data.csv', header=None)\n",
    "    \n",
    "    # Get the number of each activity (the last column in the dataset)\n",
    "    y_train = df_training[df_training.shape[1] - 1].values\n",
    "    # Labels should start from 0 in sklearn\n",
    "    y_train = y_train - 1\n",
    "    # Change the type to integer\n",
    "    y_train = y_train.astype(int)\n",
    "    # Drop the last column in the dataset\n",
    "    df_training = df_training.drop([df_training.shape[1] - 1], axis=1)\n",
    "    # Get values for model training\n",
    "    X_train = df_training.values\n",
    "    print (\"training done \")\n",
    "    \n",
    "    # Do the same as training for testing\n",
    "    y_test = df_testing[df_testing.shape[1] - 1].values\n",
    "    y_test = y_test - 1\n",
    "    y_test = y_test.astype(int)\n",
    "    df_testing = df_testing.drop([df_testing.shape[1] - 1], axis=1)\n",
    "    X_test = df_testing.values\n",
    "    print (\"testing done\")\n",
    "\n",
    "    # Feature normalization for improving the performance of machine learning models. In this code, \n",
    "    # StandardScaler is used to scale original feature to be centered around zero. \n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    print(\"End feature normalisation\")\n",
    "\n",
    "    # Build KNN classifier, in this code\n",
    "    knn = KNeighborsClassifier(n_neighbors=4)\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluation. when we train a machine learning model on training set, we should evaluate its performance on testing set.\n",
    "    # We could evaluate the model by different metrics. Firstly, we could calculate the classification accuracy.\n",
    "    # In this code, when n_neighbors is set to 3, the accuracy achieves 0.753.\n",
    "    # when n_neighbors is set to 4, the accuracy achieve 0.797\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "    # We could use confusion matrix to view the classification for each activity.\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    tuned_parameters = [{'n_neighbors':[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}]\n",
    "    acc_scorer = make_scorer(accuracy_score)\n",
    "    # Using GridSearch to search for optimal KNN classifier based on tuned parameters and accuracy score\n",
    "    grid_obj  = GridSearchCV(KNeighborsClassifier(), tuned_parameters, cv=10, scoring=acc_scorer)\n",
    "    # Fit the model for training\n",
    "    grid_obj  = grid_obj .fit(X_train, y_train)\n",
    "    # Get best classifier\n",
    "    clf = grid_obj.best_estimator_\n",
    "    print (\"some results\")\n",
    "    print('best clf:', clf)\n",
    "    # Fit the classifier model\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Create predited class\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(y_pred)\n",
    "    print(y_pred.shape, y_test.shape)\n",
    "    # Print out accuracy score and confusion matrix.\n",
    "    print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # Another machine learning model: svm. In this code, we use gridsearch to find the optimial classifier\n",
    "    # It will take a long time to find the optimal classifier.\n",
    "    # the accuracy for SVM classifier with default parameters is 0.753, \n",
    "    # which is worse than KNN. The reason may be parameters of svm classifier are not optimal.  \n",
    "    # Another reason may be we only use 9 features and they are not enough to build a good svm classifier. \n",
    "    \n",
    "    # Do the same as training for testing \n",
    "    tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-1,1e-2, 1e-3, 1e-4],\n",
    "                    'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 100]},\n",
    "                {'kernel': ['linear'], 'C': [1e-3, 1e-2, 1e-1, 1, 10, 100]}]\n",
    "    acc_scorer = make_scorer(accuracy_score)\n",
    "    grid_obj  = GridSearchCV(SVC(), tuned_parameters, cv=10, scoring=acc_scorer)\n",
    "    grid_obj  = grid_obj .fit(X_train, y_train)\n",
    "    clf = grid_obj.best_estimator_\n",
    "    print('best clf:', clf)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering feature\n",
      "deal with dataset 1\n",
      "deal with dataset 2\n",
      "deal with dataset 3\n",
      "deal with dataset 4\n",
      "deal with dataset 5\n",
      "deal with dataset 6\n",
      "deal with dataset 7\n",
      "deal with dataset 8\n",
      "deal with dataset 9\n",
      "deal with dataset 10\n",
      "deal with dataset 11\n",
      "deal with dataset 12\n",
      "deal with dataset 13\n",
      "deal with dataset 14\n",
      "deal with dataset 15\n",
      "deal with dataset 16\n",
      "deal with dataset 17\n",
      "deal with dataset 18\n",
      "deal with dataset 19\n",
      "[[-7.64914274e-01 -2.10587942e-01 -5.42663803e-01  2.70547580e-02\n",
      "   8.16927305e-02  5.67641902e-02  7.14463855e+00  2.88400090e+01\n",
      "   1.85248410e+01  1.30000000e+01]]\n",
      "Final Model\n",
      "training done \n",
      "testing done\n",
      "End feature normalisation\n",
      "Accuracy:  0.7970649392194498\n",
      "[[1303    0   24    0   19    0   22    0    0    0    0    0    0]\n",
      " [   0 1368    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0 1128  200   19   21    0    0    0    0    0    0    0]\n",
      " [  24    0  171 2109   23   49    0    0    0    0    0    0    0]\n",
      " [   6    0   26   31  479  479  138   60   57    0    3   67   22]\n",
      " [  48    0   24   57  480  900  235  114   62   41    1   54   24]\n",
      " [   4    5    0   23   67   85 4841   72   19   60    0    0   80]\n",
      " [   0    0    0    3    4   36  114  677    0   95    3    0    4]\n",
      " [   0    1    0    0    3   59  208    0  641    0    0    0    0]\n",
      " [   0    0    0    0    0   19    1    0    0 2284    0    0    0]\n",
      " [   3    0    3    0   23   45    0    0   19    0 1704  603    0]\n",
      " [   0    0    0   25    0    8   19    0    0    0  437 1911    0]\n",
      " [   1    0    0    3    2   36  147   39   46   38    9    3  588]]\n"
     ]
    }
   ],
   "source": [
    "#     print(\"visualise data\")\n",
    "#     visualisation()\n",
    "#     print(\"noise removed\")\n",
    "#     removeNoise()\n",
    "print(\"Engineering feature\")\n",
    "featureEngineering()\n",
    "print(\"Final Model\")\n",
    "modelEvaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
